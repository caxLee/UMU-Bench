<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="UMU-Bench: A Comprehensive Benchmark for Understanding Multimodal Understanding - Chengye Wang, Yuyuan Li, Xiaohua Feng, Chaochao Chen, Xiaolin Zheng, Jianwei Yin">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="UMU-Bench is a comprehensive benchmark for evaluating multimodal understanding capabilities, providing standardized evaluation metrics and datasets for multimodal AI research.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="multimodal understanding, benchmark, computer vision, natural language processing, AI evaluation, multimodal AI, machine learning">
  <!-- TODO: List all authors -->
  <meta name="author" content="Chengye Wang, Yuyuan Li, Xiaohua Feng, Chaochao Chen, Xiaolin Zheng, Jianwei Yin">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630">
  <meta property="og:image:alt" content="PAPER_TITLE - Research Preview">
  <meta property="article:published_time" content="2024-01-01T00:00:00.000Z">
  <meta property="article:author" content="FIRST_AUTHOR_NAME">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="KEYWORD1">
  <meta property="article:tag" content="KEYWORD2">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="PAPER_TITLE">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="PAPER_TITLE">
  <meta name="citation_author" content="FIRST_AUTHOR_LAST, FIRST_AUTHOR_FIRST">
  <meta name="citation_author" content="SECOND_AUTHOR_LAST, SECOND_AUTHOR_FIRST">
  <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>UMU-Bench: Closing the Modality Gap in Multimodal
    Unlearning Evaluation - Chengye Wang, Yuyuan Li, Xiaohua Feng, Chaochao Chen, Xiaolin Zheng, Jianwei Yin | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "PAPER_TITLE",
    "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
    "author": [
      {
        "@type": "Person",
        "name": "FIRST_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      },
      {
        "@type": "Person",
        "name": "SECOND_AUTHOR_NAME",
        "affiliation": {
          "@type": "Organization",
          "name": "INSTITUTION_NAME"
        }
      }
    ],
    "datePublished": "2024-01-01",
    "publisher": {
      "@type": "Organization",
      "name": "CONFERENCE_OR_JOURNAL_NAME"
    },
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["KEYWORD1", "KEYWORD2", "KEYWORD3", "machine learning", "computer vision"],
    "abstract": "FULL_ABSTRACT_TEXT_HERE",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>


  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">
              <div style="display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
                <span style="font-size: 2rem; margin-right: 0.5rem;">ðŸŒ‰</span>
                <span style="color: #2563eb; font-weight: 800;">UMU-Bench:</span>
              </div>
              <div>A Comprehensive Benchmark for Understanding Multimodal Understanding</div>
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Authors with affiliations -->
              <span class="author-block">
                <a href="#" target="_blank">Chengye Wang</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=v4e49qEAAAAJ&hl=zh-CN&oi=ao" target="_blank">Yuyuan Li</a><sup>2</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=-Iesa-UAAAAJ" target="_blank">Xiaohua Feng</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=qZTMyzwAAAAJ" target="_blank">Chaochao Chen</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=MY23M60AAAAJ" target="_blank">Xiaolin Zheng</a><sup>1,*</sup>,</span>
                  <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=zh-CN&user=0s1A5fwAAAAJ" target="_blank">Jianwei Yin</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                <!-- Institution affiliations -->
                <span class="author-block">
                  <sup>1</sup>Zhejiang University, <sup>2</sup>Hangzhou Dianzi University<br>
                  NeurIPS 2025
                </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://neurips.cc/virtual/2025/poster/121699" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="https://github.com/QDRhhhh/UMU-bench" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>GitHub</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/chengyewang/UMU-bench/discussions" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-huggingface">ðŸ¤—</i>
                  </span>
                  <span>Hugging Face</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-three-quarters">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p style="color: var(--text-primary); font-size: 1.1rem; line-height: 1.8;">
            Although Multimodal Large Language Models (MLLMs) have advanced numerous fields, their training on extensive multimodal datasets introduces significant privacy concerns, prompting the necessity for effective unlearning methods. However, current multimodal unlearning approaches often directly adapt techniques from unimodal contexts, largely overlooking the critical issue of modality alignment, i.e., consistently removing knowledge across both unimodal and multimodal settings. To close this gap, we introduce <strong>UMU-Bench</strong>, a unified benchmark specifically targeting modality misalignment in multimodal unlearning. <strong>UMU-Bench</strong> consists of a meticulously curated dataset featuring 653 individual profiles, each described with both unimodal and multimodal knowledge. Additionally, novel tasks and evaluation metrics focusing on modality alignment are introduced, facilitating a comprehensive analysis of unimodal and multimodal unlearning effectiveness. Through extensive experimentation with state-of-the-art unlearning algorithms on <strong>UMU-Bench</strong>, we demonstrate prevalent modality misalignment issues in existing methods. These findings underscore the critical need for novel multimodal unlearning approaches explicitly considering modality alignment. The code and data are publicly available at <a href="https://github.com/QDRhhhh/UMU-bench" target="_blank">https://github.com/QDRhhhh/UMU-bench</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Benchmark -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column" style="max-width: 100%;">
        <h2 class="title is-3 has-text-left">Benchmark</h2>
        <div class="content">
          <!-- TODO: Add benchmark description and details -->
         <p>
          In this paper, we introduce <strong>UMU-Bench</strong>, a knowledge-based benchmark that achieves a balance between unimodal and multimodal data, designed to address both aspects of unlearning. 
         </p>
          <div class="has-text-centered">
            <img src="static/images/Figure3.png" alt="Dataset composition and structure" style="max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1);" />
          </div>
          <!-- TODO: Add benchmark features, metrics, and evaluation criteria -->
          
          
      </div>
  </div>
</div>
</div>
</section>
<!-- End benchmark -->

<!-- Datasets -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column" style="max-width: 100%;">
        <h2 class="title is-3 has-text-left">Datasets</h2>
        <div class="content">
          <p>
            The dataset is composed of 500 fictitious individuals and 153 real individuals, each with a rich profile. Each profile contains various knowledge, including personal information such as images, names, birthplaces, birthdates, occupations and more. These profiles cover a broad spectrum of knowledge, encompassing 70 countries, 270 regions, birthdates from 1950 to 2010, 145 distinct occupations, and diverse personal preferences for each individual.
          </p>


          <p>
            The evaluation of unlearning is primarily conducted from two perspectives: Unlearning Completeness (UC) and Model Utility (UT). To facilitate these evaluations, the dataset is divided into three distinct subsets: the forget set, the retain set, and the real person set.
          </p>
          
          <h4 class="title is-5">Forget Set</h4>
          <p>
            This subset is used to evaluate the UC of the model. The forget set consists of knowledge instances from 500 fictitious individuals, and the forgetting rates are configured at 5%, 10%, and 15%. These knowledge instances are specifically chosen to assess how well the model can forget particular details after unlearning. Ideally, after the unlearning process, the model should demonstrate a significant reduction in performance when tested on this subset, as it is expected to have forgotten the associated knowledge.
          </p>
          
          <h4 class="title is-5">Retain Set</h4>
          <p>
            This subset is designed to assess UT. It includes the remaining 95%, 90%, and 85% of the 500 fictitious individuals after the knowledge instances in the forget set have been removed. The retain set evaluates the model's ability to retain relevant knowledge and maintain performance on the remaining data, even after the unlearning of specific information. Ideally, the model should demonstrate minimal performance degradation on this set, suggesting that unlearning has not overly affected the model's ability to recall and utilize the retained knowledge.
          </p>
          
          <h4 class="title is-5">Real Person Set</h4>
          <p>
            This subset is also evaluated from the perspective of UT and consists of profiles of 153 real individuals. The key feature of this set is that it is independent of the forget set. It serves to evaluate the model's general performance and robustness. Since this set represents real-world data, it is crucial to test the model's ability to generalize beyond the synthetic knowledge used in the forget set. In an ideal scenario, unlearning should not adversely affect the model's performance on this set, ensuring that the model retains its utility and general capabilities after the unlearning process.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End datasets -->

<!-- Results -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column" style="max-width: 100%;">
        <h2 class="title is-3 has-text-left">Results</h2>
        <div class="content">
          <!-- TODO: Add main results and findings -->
          
          
        
          </div>
          <p>
            Performance comparison of different unlearning algorithms on the <strong>UMU-Bench</strong> dataset, using the LLaVA-1.5-7B model across three forgetting rates (5%, 10%, and 15%).
          </p>
          <div class="has-text-centered">
            <img src="static/images/result.png" alt="Experimental results" style="max-width: 100%; height: auto;" />
          </div>
          
          <p>
            <strong>Performance across three unlearning modalities:</strong> unimodal, multimodal, and mixed mode. The evaluation metric is the difference between the original model's performance and the performance of the model after unlearning.
          </p>
          <div class="has-text-centered">
            <img src="static/images/result1.png" alt="Experimental results" style="max-width: 100%; height: auto;" />
        </div>
          
          <p>
            <strong>Evaluation of modality alignment</strong> across different unlearning algorithms (GA, GD) and a range of unimodal-to-multimodal loss balancing ratios (Î± : Î²). Each subfigure illustrates performance under varying proportions for three task types (i.e., classification, cloze, and generation) across unimodal (text-only), multimodal (text + image), and hybrid (mixed) unlearning setups.
          </p>
          <p>
            The results demonstrate how different balancing ratios influence unlearning completeness and modality alignment, highlighting the trade-offs between unimodal and multimodal performance in each algorithm.
          </p>
          <div class="has-text-centered">
            <img src="static/images/result3.png" alt="Experimental results" style="max-width: 100%; height: auto;" />
        </div>
          

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End results -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="columns is-centered">
        <div class="column" style="max-width: 100%;">
          <div class="bibtex-header">
            <h2 class="title has-text-left">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
          <pre id="bibtex-code"><code>@inproceedings{NeurIPS2025,
            author = {Chengye Wang, Yuyuan Li, Xiaohua Feng, Chaochao Chen, Xiaolin Zheng, Jianwei Yin},
            booktitle = {Advances in Neural Information Processing Systems},
            title = {UMU-Bench: Closing the Modality Gap in Multimodal Unlearning Evaluation},
            year = {2025}
           }</code></pre>
        </div>
      </div>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
